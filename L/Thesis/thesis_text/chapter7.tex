%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER                              %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\chapter{Future Extensions}
\section{List of possible future extensions}
The door of parallel execution is wide open.
Although there is research done introducing parallel execution into this algorithm, there are still very interesting extensions to research.
\par
The first and probably most interesting is scalability.
Finding out the performance when we keep adding parallel CPU execution threads and GPU capabilities.
As described in this thesis, both CPU and GPU heavy servers exist in the form of service based HPC.
It would be interesting to look for the behaviour when the GPU resources available to the algorithm are taken to the next level.
With the following research question: find a certain relative sweet point between available GPU cores and input size $n$ and/or fixed size $m$.\\
A similar thing can be said about CPU cores. 
Is there a certain relative sweet point of CPU cores available versus the input size $n$ and/or fixed size $m$ and/or number of folds $v$.\\
This being said I am quiet certain that these research question are large and extend the capabilities of a single person thesis.
\par 
The second is a possible addition to the Matlab machine learning toolbox.
This is an existing and very good toolbox, where already different variations of SVM machines are present.
However, in my opinion is the optimized FS LS-SVM, a very competitive algorithm, certainly when looking at big data sets.
This would also enhance the usability of the algorithm because certain elements of the optimized FS LS-SVM algorithm are not present in the LS-SVM toolbox.
Therefore making it a large undertaking to get started with this algorithm especially when the interest is mostly use cases and not research.
\par 
The third and smallest extension is building upon this thesis to find out the performance of variations in the made assumptions:
different kernel types, bayesian parameter calculation, the different coding schemes of multi classification,...
\section{Conclusion}
These are three interesting use cases that can be the base of future research.
If the objective of the university is that this algorithm extends from academic live and makes it introduction as a competitor for other already existing algorithms, than is extension two the most interesting.
